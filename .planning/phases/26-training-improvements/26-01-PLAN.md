---
phase: 26-training-improvements
plan: 01
type: execute
---

<objective>
Add benchmark infrastructure and stronger training oracle for measurable AI improvement.

Purpose: Currently training uses weak MCTS (3-15 iterations) which generates poor-quality games. Strong oracle games are needed for meaningful feature correlation. Additionally, we need infrastructure to actually measure win rate.

Output: Benchmark function that measures trained AI vs baseline, oracle MCTS config separate from trained AI.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-structural-features/25-02-SUMMARY.md

@packages/ai-trainer/src/trainer.ts
@packages/ai-trainer/src/simulator.ts
@packages/ai-trainer/src/types.ts

**Tech stack available:** TypeScript, Vitest for testing
**Established patterns:** Feature evaluation, parallel simulation, MCTS bot integration
**Constraining decisions:**
- Phase 24-25: Feature templates now exist for connection and capture games
- No new dependencies allowed

**Key problem:** The current "win rate" estimate is fake:
```typescript
// Line 199-202 in trainer.ts - this is NOT a real measurement
const avgCorrelation = selectedFeatures.reduce(...);
bestWinRate = 0.5 + avgCorrelation * 0.3; // Rough estimate
```

We need actual benchmark games where trained AI plays against random baseline.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add benchmark evaluation function</name>
  <files>packages/ai-trainer/src/benchmark.ts, packages/ai-trainer/src/index.ts</files>
  <action>
Create benchmark.ts with:

1. `benchmarkAI()` function that plays trained AI vs random baseline:
   - Takes GameClass, gameType, LearnedObjective[], gameCount (default 100)
   - Runs games where player 0 uses MCTS with objectives, player 1 uses random
   - Also runs reverse (player 1 trained, player 0 random) to avoid first-player advantage bias
   - Returns { winRate, wins, losses, draws, gamesPlayed }

2. Use stronger MCTS for benchmark (100 iterations minimum) - this is evaluation, not training speed

3. Track actual game outcomes (check game.settings.winners against which player was trained)

Export from index.ts.

Key implementation:
- Reuse simulateSingleGame infrastructure but with per-player AI config
- Add `perPlayerAI` option: Array<{ useAI: boolean, iterations: number, objectives?: LearnedObjective[] }>
- Default behavior unchanged when perPlayerAI not provided
  </action>
  <verify>
```bash
cd packages/ai-trainer && npx tsc --noEmit
```
Passes with no type errors.
  </verify>
  <done>benchmarkAI() function exists, exported from package index, type-checks correctly</done>
</task>

<task type="auto">
  <name>Task 2: Separate oracle MCTS from training MCTS</name>
  <files>packages/ai-trainer/src/types.ts, packages/ai-trainer/src/trainer.ts</files>
  <action>
Add distinct MCTS iteration configs to TrainingConfig:

1. In types.ts, add to TrainingConfig:
   - `oracleMCTSIterations?: number` - MCTS iterations for training oracle (default 50, high for quality games)
   - `trainedMCTSIterations?: number` - MCTS iterations when using learned objectives (default 10, lower since heuristics help)
   - Keep `mctsIterations` as deprecated alias for backwards compat

2. In trainer.ts:
   - Use oracleMCTSIterations when simulating training games
   - Use trainedMCTSIterations when objectives exist
   - Update DEFAULT_TRAINING_CONFIG with reasonable defaults

3. In trainer.ts train() method, after training completes:
   - Import benchmarkAI
   - Run benchmark with learned objectives
   - Replace fake win rate estimate with actual measured win rate
   - Update metadata.finalWinRate with real value

The training loop should report: "Iteration X complete. Benchmarking..." then actual win rate.
  </action>
  <verify>
```bash
cd packages/ai-trainer && npx tsc --noEmit
npm run build -w packages/ai-trainer
```
Both pass.
  </verify>
  <done>Oracle vs trained MCTS separated, benchmark runs after training, real win rate reported in metadata</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build -w packages/ai-trainer` succeeds
- [ ] `benchmarkAI` function exported from package
- [ ] `oracleMCTSIterations` and `trainedMCTSIterations` in TrainingConfig
- [ ] trainer.ts calls benchmarkAI and uses real win rate
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors
- Real win rate measurement instead of fake estimate
</success_criteria>

<output>
After completion, create `.planning/phases/26-training-improvements/26-01-SUMMARY.md`
</output>
