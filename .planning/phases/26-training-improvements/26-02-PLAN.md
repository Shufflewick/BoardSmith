---
phase: 26-training-improvements
plan: 02
type: tdd
---

<objective>
Implement µ+λ evolution strategy for feature weight optimization.

Purpose: Current training just converts correlation to weights with a linear formula. Evolution allows weights to be refined through selection pressure - keep combinations that actually win games, mutate to explore nearby weight spaces.

Output: Weight evolution module with mutate, crossover, and select functions tested via TDD.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/26-training-improvements/26-01-PLAN.md

@packages/ai-trainer/src/types.ts
@packages/ai-trainer/src/analyzer.ts

**Tech stack:** TypeScript, Vitest for testing
**Established patterns:** LearnedObjective with featureId + weight

**Evolution Strategy (µ+λ):**
- µ (mu) = parent population size (e.g., 5)
- λ (lambda) = offspring population size (e.g., 20)
- Each generation: mutate parents → benchmark offspring → select top µ

**Weight mutation approach:**
- Gaussian perturbation: newWeight = weight + N(0, sigma)
- Sigma starts at 1.0, decays over generations
- Occasionally flip sign (explore opposite hypothesis)
- Clamp to reasonable range [-20, 20]
</context>

<feature>
  <name>Weight Evolution System</name>
  <files>packages/ai-trainer/src/evolution.ts, packages/ai-trainer/tests/evolution.test.ts</files>
  <behavior>
**mutateWeights(objectives, sigma):**
- Input: LearnedObjective[], sigma: number
- Output: LearnedObjective[] with mutated weights
- Each weight: newWeight = clamp(weight + gaussian(0, sigma), -20, 20)
- 10% chance to flip sign (explore opposite direction)
- Returns new array (immutable)

**crossoverWeights(parent1, parent2):**
- Input: two LearnedObjective[]
- Output: LearnedObjective[] child
- For each feature present in both: random choice from either parent
- Features only in one parent: include with 50% probability
- Returns new array

**selectBest(population, fitnesses, mu):**
- Input: array of LearnedObjective[][], fitness scores for each, mu count
- Output: top mu objective sets by fitness
- Sort by fitness descending, return top mu

**generateOffspring(parents, lambda, sigma):**
- Input: parent objectives, offspring count, mutation sigma
- Output: lambda offspring via mutation (and occasional crossover)
- 80% pure mutation, 20% crossover + mutation

Test cases:
- mutateWeights preserves feature IDs, changes weights within bounds
- mutateWeights with sigma=0 returns identical weights
- crossoverWeights combines features from both parents
- selectBest returns exactly mu items in fitness order
- generateOffspring returns exactly lambda items
  </behavior>
  <implementation>
1. Create evolution.ts with pure functions
2. Use seeded RNG for reproducibility (accept seed parameter)
3. Gaussian via Box-Muller transform (no external deps)
4. All functions are pure - no side effects
  </implementation>
</feature>

<verification>
```bash
cd packages/ai-trainer && npm test
```
All evolution tests pass.
</verification>

<success_criteria>
- Failing tests written and committed
- Implementation passes all tests
- Refactor complete (if needed)
- All 2-3 commits present
</success_criteria>

<output>
After completion, create `.planning/phases/26-training-improvements/26-02-SUMMARY.md` with:
- RED: What tests were written, why they failed
- GREEN: What implementation made them pass
- REFACTOR: What cleanup was done (if any)
- Commits: List of commits produced
</output>
