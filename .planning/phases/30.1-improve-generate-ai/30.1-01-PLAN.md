---
phase: 30.1-improve-generate-ai
plan: 01
type: execute
---

<objective>
Improve `/generate-ai` slash command based on lessons from Hex AI debugging in Phases 29-30.

Purpose: The current `/generate-ai` only generates objectives functions, missing critical AI components discovered during Hex development: threat response detection, playout policies, and human exploit testing.

Output: Updated generate-ai-instructions.md that guides Claude to generate complete AI configurations including objectives, threatResponseMoves, and playoutPolicy functions.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Reference implementation (Hex AI with all three hooks):**
@packages/games/hex/rules/src/ai.ts

**Current generate-ai implementation:**
@packages/cli/src/slash-command/generate-ai-instructions.md
@packages/cli/src/slash-command/generate-ai.template.md

**AI types (shows all three hooks):**
@packages/ai/src/types.ts

**Prior phase context:**
- Phase 29: playoutDepth > 0 dramatically improved AI quality (57.5% vs 40% P1 win rate)
- Phase 30: threatResponseMoves hook added for forced defensive play when opponent near winning
- Key insight: AI vs AI benchmarks missed obvious human exploits (straight-line strategies in Hex)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add threat response generation phase</name>
  <files>packages/cli/src/slash-command/generate-ai-instructions.md</files>
  <action>
Add a new "Phase 7: Threat Response" section after Phase 6 (Validation) that:

1. Explains when threat response is needed (game types where opponent can win quickly)
2. Provides the ThreatResponse interface structure from types.ts
3. Shows patterns per game type:
   - **Connection games**: Use Dijkstra shortest-path to find cells on opponent's winning path
   - **Capture games**: Detect when pieces are under attack, find defending moves
   - **Racing games**: Block opponent's progress toward goal
   - **Territory games**: Respond to invasions, protect borders

4. Provides code template for threatResponseMoves function:
   ```typescript
   export function get<GameName>ThreatResponseMoves(
     game: Game,
     playerPosition: number,
     availableMoves: BotMove[]
   ): ThreatResponse {
     // Analyze opponent's position
     // Find critical blocking moves
     // Return { moves: [...], urgent: boolean }
   }
   ```

5. Explains the urgent flag:
   - urgent=true: Opponent about to win, MUST block (MCTS only considers these moves)
   - urgent=false: Threat exists but not immediate (moves prioritized, all options evaluated)

Reference the Hex implementation as the exemplar (getHexThreatResponseMoves uses Dijkstra path reconstruction).

Update existing game type sections (Phase 3) to mention what "threats" look like for each type.
  </action>
  <verify>grep -n "threat" packages/cli/src/slash-command/generate-ai-instructions.md shows new Phase 7 section and updated game type descriptions</verify>
  <done>Phase 7 exists with threat patterns per game type, ThreatResponse template, urgent flag explanation</done>
</task>

<task type="auto">
  <name>Task 2: Add playout policy generation phase</name>
  <files>packages/cli/src/slash-command/generate-ai-instructions.md</files>
  <action>
Add a new "Phase 8: Playout Policy (Optional)" section after Phase 7 that:

1. Explains purpose: Replace random move selection during MCTS playouts with weighted selection
2. Notes this is optional but dramatically improves AI quality (random playouts = random-looking play)
3. Shows when to use:
   - Connection games: Prefer moves that shorten winning path, extend opponent's path
   - Capture games: Prefer safe moves, avoid hanging pieces
   - Territory games: Prefer moves that expand influence coherently

4. Provides code template for playoutPolicy function:
   ```typescript
   export function get<GameName>PlayoutPolicy(
     game: Game,
     playerIndex: number,
     availableMoves: BotMove[],
     rng: () => number
   ): BotMove {
     // Score each move based on heuristics
     // Use weighted-random selection (not deterministic!)
     // Return selected move
   }
   ```

5. Shows the weighted selection pattern from Hex (softmax-like weighting):
   - Score moves based on game-specific heuristics
   - Convert scores to weights using exp(score - maxScore)
   - Select weighted-random using provided rng

6. Critical warning: "Playout policy MUST use weighted-random selection via the rng parameter. Deterministic selection breaks MCTS exploration and produces weak play."

Reference the Hex implementation (getHexPlayoutPolicy) as the exemplar.
  </action>
  <verify>grep -n "playoutPolicy" packages/cli/src/slash-command/generate-ai-instructions.md shows new Phase 8 section</verify>
  <done>Phase 8 exists with playout policy purpose, when to use, code template, weighted selection pattern, and critical warning about rng usage</done>
</task>

<task type="auto">
  <name>Task 3: Add human exploit testing section</name>
  <files>packages/cli/src/slash-command/generate-ai-instructions.md</files>
  <action>
Update Phase 6 (Validation) to add human exploit testing guidance:

1. Add new subsection "### Human Exploit Testing" after the TypeScript verification step

2. Explain the problem:
   "AI vs AI benchmarks can miss obvious human exploits. In Hex, the AI played well against itself but failed to block straight-line strategies a human would easily spot."

3. Provide testing suggestions:
   - "Play 3-5 games manually against the AI"
   - "Try simple, obvious strategies first (straight lines, central control, aggressive attacks)"
   - "If the AI consistently fails to respond to an obvious threat, add threat detection"

4. Add a checklist:
   - [ ] AI blocks obvious winning threats
   - [ ] AI doesn't make random-looking moves late game
   - [ ] AI vs AI games are contested (not one-sided blowouts)
   - [ ] AI responds to central/key position control

5. Suggest iterating:
   "If human testing reveals exploits, consider adding:
   - threatResponseMoves (for blocking)
   - playoutPolicy (for strategic move selection)
   - Additional objectives (for position evaluation)"
  </action>
  <verify>grep -n "Human Exploit" packages/cli/src/slash-command/generate-ai-instructions.md shows new testing section</verify>
  <done>Phase 6 updated with Human Exploit Testing subsection including the Hex lesson, manual testing suggestions, and iteration guidance</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Phase 7 (Threat Response) exists with ThreatResponse interface, game type patterns, code template
- [ ] Phase 8 (Playout Policy) exists with weighted selection pattern, rng warning, code template
- [ ] Phase 6 (Validation) includes Human Exploit Testing subsection
- [ ] All three AIConfig hooks documented: objectives, threatResponseMoves, playoutPolicy
- [ ] Hex implementation referenced as exemplar for threat response and playout policy
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- /generate-ai instructions now guide generation of complete AI configurations
- Human exploit testing emphasized in validation phase
</success_criteria>

<output>
After completion, create `.planning/phases/30.1-improve-generate-ai/30.1-01-SUMMARY.md`:

# Phase 30.1 Plan 01: Improve /generate-ai Summary

**[Substantive one-liner]**

## Accomplishments
- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified
- `packages/cli/src/slash-command/generate-ai-instructions.md` - Added threat response, playout policy, and human testing guidance

## Decisions Made
[Key decisions and rationale]

## Issues Encountered
[Problems and resolutions, or "None"]

## Next Step
Phase 30.1 complete, ready for Phase 31: trajectory-objectives
</output>
